{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 23 Data Generation\n",
    "\n",
    "This notebook processes raw two-point correlation experimental data to generate processed data for Figure 1.\n",
    "\n",
    "## Process:\n",
    "1. Load raw experimental data from UUIDs (high and low field)\n",
    "2. Process data (extract chi1 and chi12 correlation parameters)\n",
    "3. Save processed data for plotting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common utilities\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "\n",
    "# Add common directory to path (works in Jupyter notebooks)\n",
    "common_path = Path().resolve().parent / 'common_scripts'\n",
    "sys.path.insert(0, str(common_path))\n",
    "import correlation_fun\n",
    "from raw_data_loader import load_raw_data_by_uuid\n",
    "from data_saver import save_figure_data\n",
    "import data_processor as processor\n",
    "import numpy as np\n",
    "\n",
    "# Reload modules to ensure latest changes\n",
    "importlib.reload(processor)\n",
    "importlib.reload(correlation_fun)\n",
    "\n",
    "\n",
    "data_path = Path().resolve().parent / 'data'\n",
    "# Data path\n",
    "\n",
    "figure_path = Path().resolve().parent / 'figures'\n",
    "\n",
    "\n",
    "# Alias for convenience\n",
    "load_by_uuid = load_raw_data_by_uuid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Raw Data\n",
    "\n",
    "Load raw experimental data for two-point correlation measurements (high and low field).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading low field data...\n",
      "✓ Loaded S1\n",
      "✓ Loaded S2\n",
      "✓ Loaded S12\n",
      "\n",
      "Loading high field data...\n",
      "✓ Loaded S1\n",
      "✓ Loaded S2\n",
      "✓ Loaded S12\n"
     ]
    }
   ],
   "source": [
    "# Low field data UUIDs\n",
    "# S1: Reference position\n",
    "# S2: Single-dot measurements at different positions\n",
    "# S12: Two-point correlation measurements\n",
    "low_field_uuids = {\n",
    "    \"S1\": \"1730672289460108893\",  # Reference position\n",
    "    \"S2\": \"1730750147026108893\",  # Single-dot measurements\n",
    "    \"S12\": \"1730827675550108893\"  # Two-point correlations (CORRECTED)\n",
    "}\n",
    "\n",
    "# High field data UUIDs (75mV)\n",
    "high_field_uuids = {\n",
    "    \"S1\": \"1727225883960108893\",  # Reference position\n",
    "    \"S2\": \"1727226751312108893\",  # Single-dot measurements\n",
    "    \"S12\": \"1727224984279108893\"  # Two-point correlations\n",
    "}\n",
    "\n",
    "# Load low field data\n",
    "print(\"Loading low field data...\")\n",
    "data_low = {}\n",
    "for key, uuid in low_field_uuids.items():\n",
    "    dataset = load_by_uuid(uuid, data_path)\n",
    "    if dataset is None:\n",
    "        print(f\"Warning: Failed to load {key} with UUID {uuid}\")\n",
    "    else:\n",
    "        data_low[key] = dataset\n",
    "        print(f\"✓ Loaded {key}\")\n",
    "\n",
    "# Load high field data\n",
    "print(\"\\nLoading high field data...\")\n",
    "data_high = {}\n",
    "for key, uuid in high_field_uuids.items():\n",
    "    dataset = load_by_uuid(uuid, data_path)\n",
    "    if dataset is None:\n",
    "        print(f\"Warning: Failed to load {key} with UUID {uuid}\")\n",
    "    else:\n",
    "        data_high[key] = dataset\n",
    "        print(f\"✓ Loaded {key}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low field data shapes:\n",
      "  S1 (reference): (200,)\n",
      "  S2 (single-dot): (18, 180)\n",
      "  S12 (two-point): (18, 180)\n",
      "  Wait times (S12/S2): (180,)\n",
      "  Wait times (S1 ref): (200,)\n",
      "  Positions: 18\n",
      "  Shuttling positions: [ 0.          1.05882353  2.11764706  3.17647059  4.23529412  5.29411765\n",
      "  6.35294118  7.41176471  8.47058824  9.52941176 10.58823529 11.64705882\n",
      " 12.70588235 13.76470588 14.82352941 15.88235294 16.94117647 18.        ]\n"
     ]
    }
   ],
   "source": [
    "# Extract low field data\n",
    "# Low field uses m1_3() measurement channel (based on original notebook)\n",
    "S1_low = data_low[\"S1\"].m1_3()    # Reference position measurements\n",
    "S2_low = data_low[\"S2\"].m1_3()    # Shuttled position measurements\n",
    "S12_low = data_low[\"S12\"].m1_3()  # Cross-correlation measurements\n",
    "\n",
    "# Extract time arrays\n",
    "wait_times12 = data_low[\"S12\"].m1_3.y()      # Wait times for S12 and S2\n",
    "wait_times1 = data_low[\"S2\"].m1_3.y()\n",
    "wait_times_ref_low = data_low[\"S1\"].m1_3.x()   # Wait times for reference S1\n",
    "\n",
    "# Calculate shuttling distances in nm\n",
    "# Formula: (20 - position) * 0.06 * 180\n",
    "shuttling_distances_low = (20 - data_low[\"S12\"].m1_3.x()) * 0.06 * 180\n",
    "\n",
    "# Use wait_times12 for S12 and S2, wait_times_ref_low for S1\n",
    "wait_times_low = wait_times1\n",
    "\n",
    "print(f\"Low field data shapes:\")\n",
    "print(f\"  S1 (reference): {S1_low.shape}\")\n",
    "print(f\"  S2 (single-dot): {S2_low.shape}\")\n",
    "print(f\"  S12 (two-point): {S12_low.shape}\")\n",
    "print(f\"  Wait times (S12/S2): {wait_times_low.shape}\")\n",
    "print(f\"  Wait times (S1 ref): {wait_times_ref_low.shape}\")\n",
    "print(f\"  Positions: {len(shuttling_distances_low)}\")\n",
    "print(f\"  Shuttling positions: {data_low['S12'].m1_3.x()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 19 chi1 (single-dot) positions\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(processor)\n",
    "\n",
    "# Process low field: chi1 (single-dot) parameters\n",
    "params_chi1_low = []\n",
    "guess = {\n",
    "    \"T2\": 5000,\n",
    "    \"n\": 1.5,\n",
    "    \"freq\": 1e-2\n",
    "}\n",
    "\n",
    "data1_ref = S1_low.copy()\n",
    "if len(data1_ref.shape) > 1:\n",
    "    data1_ref = data1_ref[0]\n",
    "data1_ref = data1_ref - np.mean(data1_ref)\n",
    "\n",
    "A, A_err, T2, T2_err, n, n_err, t_data, envelope, y_detrended, phi, f, f_err, B = \\\n",
    "    processor.get_fit_hilbert_drift(data1_ref, wait_times_ref_low, guess, use_hilbert=False)\n",
    "sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "params_chi1_low.append({\n",
    "    'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "    'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "    'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': 0,\n",
    "    'envelope': envelope, 't_data': t_data\n",
    "})\n",
    "\n",
    "for i in range(S2_low.shape[0]):\n",
    "    if len(S2_low.shape) == 2:\n",
    "        data2 = S2_low[i, :].copy()\n",
    "    else:\n",
    "        data2 = S2_low[i].copy()\n",
    "    data2 = data2 - np.mean(data2)\n",
    "    \n",
    "    A, A_err, T2, T2_err, n, n_err, t_data, envelope, y_detrended, phi, f, f_err, B = \\\n",
    "        processor.get_fit_hilbert_drift(data2, wait_times_low, guess, use_hilbert=False)\n",
    "    sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "    params_chi1_low.append({\n",
    "        'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "        'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "        'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': shuttling_distances_low[i],\n",
    "        'envelope': envelope, 't_data': t_data\n",
    "    })\n",
    "\n",
    "print(f\"Processed {len(params_chi1_low)} chi1 (single-dot) positions\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 18 chi12 (two-point) positions\n"
     ]
    }
   ],
   "source": [
    "# Process low field: chi12 (two-point) parameters\n",
    "params_chi12_low = []\n",
    "\n",
    "for i in range(S12_low.shape[0]):\n",
    "    if len(S12_low.shape) == 2:\n",
    "        data12 = S12_low[i, :].copy()\n",
    "    else:\n",
    "        data12 = S12_low[i].copy()\n",
    "    data12 = data12 - np.mean(data12)\n",
    "    \n",
    "    T2_xn = params_chi1_low[0][\"T2\"]\n",
    "    alpha_xn = params_chi1_low[0][\"n\"]\n",
    "    T2_xm = params_chi1_low[i+1][\"T2\"]\n",
    "    alpha_xm = params_chi1_low[i+1][\"n\"]\n",
    "  \n",
    "    A, A_err, T2, T2_err, n, n_err, t_data, envelope, y_detrended, phi, f, f_err, B = \\\n",
    "        processor.get_fit_hilbert_drift(data12, wait_times12, guess, use_hilbert=False)\n",
    "    \n",
    "    T12eff, alpha12eff = correlation_fun.calculate_chi_with_background_corrected(\n",
    "        [T2_xn, T2_xm], [alpha_xn, alpha_xm], 2, np.eye(2), 1e99, 2\n",
    "    )\n",
    "    \n",
    "    sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "    rnm, rnm_err, chi_dc_fit = processor.fit_rnm(wait_times12, 2*envelope/A, T2_xn, T2_xm, alpha_xn, alpha_xm, 0, [-2,2])\n",
    "    \n",
    "    params_chi12_low.append({\n",
    "        'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "        'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "        'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': shuttling_distances_low[i],\n",
    "        'rnm': rnm, 'rnm_err': rnm_err, 'T12eff': T12eff, 'alpha12eff': alpha12eff,\n",
    "        'envelope': envelope, 't_data': t_data\n",
    "    })\n",
    "\n",
    "print(f\"Processed {len(params_chi12_low)} chi12 (two-point) positions\")\n",
    "\n",
    "params_chi12_low = sorted(params_chi12_low, key=lambda p: p['x'])\n",
    "params_chi1_low = sorted(params_chi1_low, key=lambda d: d['x'])\n",
    "params_low = {\"chi1\": params_chi1_low, \"chi12\": params_chi12_low}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High field data shapes:\n",
      "  S1 (reference): (70,)\n",
      "  S2 (single-dot): (8, 70)\n",
      "  S12 (two-point): (8, 70)\n",
      "  Wait times (S12/S2): (70,)\n",
      "  Wait times (S1 ref): (70,)\n",
      "  Positions: 8\n"
     ]
    }
   ],
   "source": [
    "# Extract high field data\n",
    "m1_S1_high = data_high[\"S1\"].m1_5()\n",
    "m1_S2_high = data_high[\"S2\"].m1_5()\n",
    "m1_S12_high = data_high[\"S12\"].m1_5()\n",
    "\n",
    "S1_high = m1_S1_high[0, 0]\n",
    "S2_high = m1_S2_high[0]\n",
    "S12_high = m1_S12_high[0]\n",
    "\n",
    "wait_times_high = data_high[\"S12\"].m1_5.k()\n",
    "wait_times_ref_high = data_high[\"S1\"].m1_5.k()\n",
    "\n",
    "CV_times = np.array([1., 6.28571429, 11.57142857, 16.85714286, 22.14285714, 27.42857143, 32.71428571, 38.])\n",
    "shuttling_distances_high = 216 - CV_times * 0.03 * 180\n",
    "\n",
    "print(f\"High field data shapes:\")\n",
    "print(f\"  S1 (reference): {S1_high.shape}\")\n",
    "print(f\"  S2 (single-dot): {S2_high.shape}\")\n",
    "print(f\"  S12 (two-point): {S12_high.shape}\")\n",
    "print(f\"  Wait times (S12/S2): {wait_times_high.shape}\")\n",
    "print(f\"  Wait times (S1 ref): {wait_times_ref_high.shape}\")\n",
    "print(f\"  Positions: {len(shuttling_distances_high)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process high field: chi1 (single-dot) parameters\n",
    "params_chi1_high = []\n",
    "\n",
    "data1_ref = S1_high.copy()\n",
    "data1_ref -= np.mean(data1_ref)\n",
    "guess_ref = {\"T2\": 3000, \"n\": 1.5, \"freq\": 0.01}\n",
    "A, A_err, T2, T2_err, n, n_err, t_data, y_detrended, envelope, phi, f, f_err, B = \\\n",
    "    processor.get_fit_hilbert_drift(data1_ref, wait_times_ref_high, guess_ref, use_hilbert=True, window_size=10)\n",
    "sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "params_chi1_high.append({\n",
    "    'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "    'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "    'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': 0,\n",
    "    'envelope': envelope, 't_data': t_data\n",
    "})\n",
    "\n",
    "for i in range(S2_high.shape[0]):\n",
    "    data2 = S2_high[i, :].copy()\n",
    "    data2 -= np.mean(data2)\n",
    "    guess = {\"T2\": 4000, \"n\": 2, \"freq\": 0.01}\n",
    "    A, A_err, T2, T2_err, n, n_err, t_data, y_detrended, envelope, phi, f, f_err, B = \\\n",
    "        processor.get_fit_hilbert_drift(data2, wait_times_high, guess, use_hilbert=True, window_size=20)\n",
    "    sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "    params_chi1_high.append({\n",
    "        'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "        'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "        'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': shuttling_distances_high[i],\n",
    "        'envelope': envelope, 't_data': t_data\n",
    "    })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8 chi12 (two-point) positions\n"
     ]
    }
   ],
   "source": [
    "# Process high field: chi12 (two-point) parameters\n",
    "params_chi12_high = []\n",
    "\n",
    "for i in range(S12_high.shape[0]):\n",
    "    data12 = S12_high[i, :].copy()\n",
    "    data12 -= np.mean(data12)\n",
    "    \n",
    "    T2_xn = params_chi1_high[0][\"T2\"]\n",
    "    alpha_xn = params_chi1_high[0][\"n\"]\n",
    "    T2_xm = params_chi1_high[i+1][\"T2\"]\n",
    "    alpha_xm = params_chi1_high[i+1][\"n\"]\n",
    "    freq = params_chi1_high[0][\"freq\"]\n",
    "    \n",
    "    guess = {\"T2\": 4000, \"n\": 2, \"freq\": 0.01}\n",
    "    A, A_err, T2, T2_err, n, n_err, t_data, envelope, y_detrended, phi, f, f_err, B = \\\n",
    "        processor.get_fit_hilbert_drift(data12, wait_times_high, guess, use_hilbert=True, window_size=20)\n",
    "    \n",
    "    T12eff, alpha12eff = correlation_fun.calculate_chi_with_background_corrected(\n",
    "        [T2_xn, T2_xm], [alpha_xn, alpha_xm], 2, np.eye(2), 1e99, 2\n",
    "    )\n",
    "    \n",
    "    sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "    rnm, rnm_err, chi_dc_fit = processor.fit_rnm(wait_times_high, 2*envelope/A, T2_xn, T2_xm, alpha_xn, alpha_xm, 0, [-2,2])\n",
    "\n",
    "    params_chi12_high.append({\n",
    "        'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "        'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "        'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': shuttling_distances_high[i],\n",
    "        'rnm': rnm, 'rnm_err': rnm_err, 'T12eff': T12eff, 'alpha12eff': alpha12eff,\n",
    "        'envelope': envelope, 't_data': t_data\n",
    "    })\n",
    "\n",
    "print(f\"Processed {len(params_chi12_high)} chi12 (two-point) positions\")\n",
    "\n",
    "params_chi1_high = sorted(params_chi1_high, key=lambda d: d['x'])\n",
    "params_chi12_high = sorted(params_chi12_high, key=lambda d: d['x'])\n",
    "params_high = {\"chi1\": params_chi1_high, \"chi12\": params_chi12_high}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved fig23 data to /Users/krzywdaja/Documents/spatial-correlations-conveyor/data_analysis/protection_code_repo/processed_data/fig23/fig23_low.pkl\n",
      "  Metadata saved to /Users/krzywdaja/Documents/spatial-correlations-conveyor/data_analysis/protection_code_repo/processed_data/fig23/fig23_low.json\n",
      "✓ Saved fig23 data to /Users/krzywdaja/Documents/spatial-correlations-conveyor/data_analysis/protection_code_repo/processed_data/fig23/fig23_high_75mV.pkl\n",
      "  Metadata saved to /Users/krzywdaja/Documents/spatial-correlations-conveyor/data_analysis/protection_code_repo/processed_data/fig23/fig23_high_75mV.json\n",
      "✓ Data saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save low field data\n",
    "save_figure_data(\n",
    "    params_low,\n",
    "    figure_number=\"fig2_3\",\n",
    "    filename=\"fig23_low.pkl\",\n",
    "    metadata={\n",
    "        \"field\": \"low\",\n",
    "        \"description\": \"Two-point correlation parameters for low field\",\n",
    "        \"shuttling_distances\": shuttling_distances_low.tolist()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save high field data (75mV)\n",
    "save_figure_data(\n",
    "    params_high,\n",
    "    figure_number=\"fig2_3\",\n",
    "    filename=\"fig23_high_75mV.pkl\",\n",
    "    metadata={\n",
    "        \"field\": \"high\",\n",
    "        \"voltage\": \"75mV\",\n",
    "        \"description\": \"Two-point correlation parameters for high field (75mV)\",\n",
    "        \"shuttling_distances\": shuttling_distances_high.tolist()\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"✓ Data saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Gather all relevant data for high field comprehensive plot\n",
    "data_to_save = {\n",
    "    \"params_high\": params_high,\n",
    "    \"shuttling_distances_high\": shuttling_distances_high,\n",
    "    \"S1_high\": S1_high.tolist() if hasattr(S1_high, \"tolist\") else S1_high,\n",
    "    \"S2_high\": S2_high.tolist() if hasattr(S2_high, \"tolist\") else S2_high,\n",
    "    \"S12_high\": S12_high.tolist() if hasattr(S12_high, \"tolist\") else S12_high,\n",
    "    \"wait_times_high\": wait_times_high.tolist() if hasattr(wait_times_high, \"tolist\") else wait_times_high,\n",
    "    \"wait_times_ref_high\": wait_times_ref_high.tolist() if hasattr(wait_times_ref_high, \"tolist\") else wait_times_ref_high,\n",
    "}\n",
    "\n",
    "save_dir = os.path.join(\"..\", \"processed_data\", \"fig19_20\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"data_high.json\")\n",
    "\n",
    "# Convert any numpy types to native python types so JSON serializes cleanly\n",
    "def make_json_serializable(d):\n",
    "    import numpy as np\n",
    "    if isinstance(d, dict):\n",
    "        return {k: make_json_serializable(v) for k, v in d.items()}\n",
    "    elif isinstance(d, np.ndarray):\n",
    "        return d.tolist()\n",
    "    elif isinstance(d, (np.integer, np.floating)):\n",
    "        return d.item()\n",
    "    elif isinstance(d, list):\n",
    "        return [make_json_serializable(x) for x in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(make_json_serializable(data_to_save), f, indent=2)\n",
    "\n",
    "print(f\"Saved high-field comprehensive plot data to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Gather all relevant data for low field comprehensive plot\n",
    "data_to_save = {\n",
    "    \"params_low\": params_low,\n",
    "    \"shuttling_distances_low\": shuttling_distances_low,\n",
    "    \"S1_low\": S1_low.tolist() if hasattr(S1_low, \"tolist\") else S1_low,\n",
    "    \"S2_low\": S2_low.tolist() if hasattr(S2_low, \"tolist\") else S2_low,\n",
    "    \"S12_low\": S12_low.tolist() if hasattr(S12_low, \"tolist\") else S12_low,\n",
    "    \"wait_times_low\": wait_times_low.tolist() if hasattr(wait_times_low, \"tolist\") else wait_times_low,\n",
    "    \"wait_times_ref_low\": wait_times_ref_low.tolist() if hasattr(wait_times_ref_low, \"tolist\") else wait_times_ref_low,\n",
    "    \"wait_times12\": wait_times12.tolist() if hasattr(wait_times12, \"tolist\") else wait_times12,\n",
    "}\n",
    "\n",
    "save_dir = os.path.join(\"..\", \"processed_data\", \"fig19_20\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"data_low.json\")\n",
    "\n",
    "# Convert any numpy types to native python types so JSON serializes cleanly\n",
    "def make_json_serializable(d):\n",
    "    import numpy as np\n",
    "    if isinstance(d, dict):\n",
    "        return {k: make_json_serializable(v) for k, v in d.items()}\n",
    "    elif isinstance(d, np.ndarray):\n",
    "        return d.tolist()\n",
    "    elif isinstance(d, (np.integer, np.floating)):\n",
    "        return d.item()\n",
    "    elif isinstance(d, list):\n",
    "        return [make_json_serializable(x) for x in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(make_json_serializable(data_to_save), f, indent=2)\n",
    "\n",
    "print(f\"Saved low-field comprehensive plot data to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variability study\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import plotting utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data for 65mV, 75mV, 85mV...\n",
      "\n",
      "Loading 65mV data...\n",
      "✓ Loaded S12\n",
      "✓ Loaded S1\n",
      "✓ Loaded S2\n",
      "\n",
      "Loading 75mV data...\n",
      "✓ Loaded S12\n",
      "✓ Loaded S1\n",
      "✓ Loaded S2\n",
      "\n",
      "Loading 85mV data...\n",
      "✓ Loaded S12\n",
      "✓ Loaded S1\n",
      "✓ Loaded S2\n"
     ]
    }
   ],
   "source": [
    "# Load data for 65mV, 75mV, 85mV\n",
    "data_65mv = {\"S12\": {\"id\": 1727222345743108893}, \n",
    "             \"S1\": {\"id\": 1727223245897108893},\n",
    "             \"S2\": {\"id\": 1727224112791108893}}\n",
    "\n",
    "data_75mv = {\"S12\": {\"id\": 1727224984279108893},\n",
    "             \"S1\": {\"id\": 1727225883960108893}, \n",
    "             \"S2\": {\"id\": 1727226751312108893}}\n",
    "\n",
    "data_85mv = {\"S12\": {\"id\": 1727227620625108893},\n",
    "             \"S1\": {\"id\": 1727228520034108893}, \n",
    "             \"S2\": {\"id\": 1727229387657108893}}\n",
    "\n",
    "voltage_data_configs = [data_65mv, data_75mv, data_85mv]\n",
    "voltage_labels = ['65mV', '75mV', '85mV']\n",
    "\n",
    "# Load all voltage data\n",
    "print(\"Loading data for 65mV, 75mV, 85mV...\")\n",
    "all_voltage_data = []\n",
    "for m, config in enumerate(voltage_data_configs):\n",
    "    print(f\"\\nLoading {voltage_labels[m]} data...\")\n",
    "    voltage_data = {}\n",
    "    for key, uuid_dict in config.items():\n",
    "        uuid = uuid_dict[\"id\"]\n",
    "        dataset = load_by_uuid(uuid, data_path)\n",
    "        if dataset is None:\n",
    "            print(f\"Warning: Failed to load {key} with UUID {uuid}\")\n",
    "        else:\n",
    "            voltage_data[key] = dataset\n",
    "            print(f\"✓ Loaded {key}\")\n",
    "    all_voltage_data.append(voltage_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing 65mV data...\n",
      "  Data shapes: S1=(70,), S2=(8, 70), S12=(8, 70)\n",
      "  Positions: 8\n",
      "  Processed 9 chi1 positions\n",
      "  Processed 8 chi12 positions\n",
      "\n",
      "Processing 75mV data...\n",
      "  Data shapes: S1=(70,), S2=(8, 70), S12=(8, 70)\n",
      "  Positions: 8\n",
      "  Processed 9 chi1 positions\n",
      "  Processed 8 chi12 positions\n",
      "\n",
      "Processing 85mV data...\n",
      "  Data shapes: S1=(70,), S2=(8, 70), S12=(8, 70)\n",
      "  Positions: 8\n",
      "  Processed 9 chi1 positions\n",
      "  Processed 8 chi12 positions\n",
      "\n",
      "✓ Processed all 3 voltages\n"
     ]
    }
   ],
   "source": [
    "# Process data for each voltage using high-field processing approach\n",
    "params_all_voltages = []\n",
    "\n",
    "for m, voltage_data in enumerate(all_voltage_data):\n",
    "    print(f\"\\nProcessing {voltage_labels[m]} data...\")\n",
    "    \n",
    "    # Extract high field data (same approach as cell 9)\n",
    "    m1_S1 = voltage_data[\"S1\"].m1_5()\n",
    "    m1_S2 = voltage_data[\"S2\"].m1_5()\n",
    "    m1_S12 = voltage_data[\"S12\"].m1_5()\n",
    "\n",
    "    S1 = m1_S1[0, 0]\n",
    "    S2 = m1_S2[0]\n",
    "    S12 = m1_S12[0]\n",
    "\n",
    "    wait_times = voltage_data[\"S12\"].m1_5.k()\n",
    "    wait_times_ref = voltage_data[\"S1\"].m1_5.k()\n",
    "\n",
    "    CV_times = np.array([1., 6.28571429, 11.57142857, 16.85714286, 22.14285714, 27.42857143, 32.71428571, 38.])\n",
    "    shuttling_distances = 216 - CV_times * 0.03 * 180\n",
    "\n",
    "    print(f\"  Data shapes: S1={S1.shape}, S2={S2.shape}, S12={S12.shape}\")\n",
    "    print(f\"  Positions: {len(shuttling_distances)}\")\n",
    "    \n",
    "    # Process chi1 (single-dot) parameters (same approach as cell 11)\n",
    "    params_chi1 = []\n",
    "\n",
    "    data1_ref = S1.copy()\n",
    "    data1_ref -= np.mean(data1_ref)\n",
    "    guess_ref = {\"T2\": 3000, \"n\": 2, \"freq\": 0.01}\n",
    "    A, A_err, T2, T2_err, n, n_err, t_data, y_detrended, envelope, phi, f, f_err, B = \\\n",
    "        processor.get_fit_hilbert_drift(data1_ref, wait_times_ref, guess_ref, use_hilbert=True, window_size=10)\n",
    "    sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "    params_chi1.append({\n",
    "        'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "        'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "        'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': 0,\n",
    "        'envelope': envelope, 't_data': t_data\n",
    "    })\n",
    "\n",
    "    for i in range(S2.shape[0]):\n",
    "        data2 = S2[i, :].copy()\n",
    "        data2 -= np.mean(data2)\n",
    "        guess = {\"T2\": 4000, \"n\": 2, \"freq\": 0.01}\n",
    "        A, A_err, T2, T2_err, n, n_err, t_data, y_detrended, envelope, phi, f, f_err, B = \\\n",
    "            processor.get_fit_hilbert_drift(data2, wait_times, guess, use_hilbert=True, window_size=20)\n",
    "        sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "        params_chi1.append({\n",
    "            'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "            'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "            'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': shuttling_distances[i],\n",
    "            'envelope': envelope, 't_data': t_data\n",
    "        })\n",
    " \n",
    "    print(f\"  Processed {len(params_chi1)} chi1 positions\")\n",
    "    \n",
    "    # Process chi12 (two-point) parameters (same approach as cell 14)\n",
    "    params_chi12 = []\n",
    "\n",
    "    for i in range(S12.shape[0]):\n",
    "        data12 = S12[i, :].copy()\n",
    "        data12 -= np.mean(data12)\n",
    "        \n",
    "        T2_xn = params_chi1[0][\"T2\"]\n",
    "        alpha_xn = params_chi1[0][\"n\"]\n",
    "        T2_xm = params_chi1[i+1][\"T2\"]\n",
    "        alpha_xm = params_chi1[i+1][\"n\"]\n",
    "        freq = params_chi1[0][\"freq\"]\n",
    "        \n",
    "        guess = {\"T2\": 4000, \"n\": 2, \"freq\": 0.01}\n",
    "        A, A_err, T2, T2_err, n, n_err, t_data, envelope, y_detrended, phi, f, f_err, B = \\\n",
    "            processor.get_fit_hilbert_drift(data12, wait_times, guess, use_hilbert=True, window_size=20)\n",
    "        \n",
    "        T12eff, alpha12eff = correlation_fun.calculate_chi_with_background_corrected(\n",
    "            [T2_xn, T2_xm], [alpha_xn, alpha_xm], 2, np.eye(2), 1e99, 2\n",
    "        )\n",
    "        \n",
    "        sig = 1 / np.sqrt(2) / np.pi / T2\n",
    "        rnm, rnm_err, chi_dc_fit = processor.fit_rnm(wait_times, 2*envelope/A, T2_xn, T2_xm, alpha_xn, alpha_xm, 0, [-2,2])\n",
    "\n",
    "        params_chi12.append({\n",
    "            'A': A, 'T2': T2, 'n': n, 'phi': phi, 'freq': f, 'B': B,\n",
    "            'sig': sig, 'dsig': T2_err / np.sqrt(2) / np.pi / T2**2,\n",
    "            'dT': T2_err, 'dn': n_err, 'dfreq': f_err, 'x': shuttling_distances[i],\n",
    "            'rnm': rnm, 'rnm_err': rnm_err, 'T12eff': T12eff, 'alpha12eff': alpha12eff,\n",
    "            'envelope': envelope, 't_data': t_data\n",
    "        })\n",
    "\n",
    "\n",
    "    print(f\"  Processed {len(params_chi12)} chi12 positions\")\n",
    "\n",
    "    params_chi1 = sorted(params_chi1, key=lambda d: d['x'])\n",
    "    params_chi12 = sorted(params_chi12, key=lambda d: d['x'])\n",
    "    params_voltage = {\"chi1\": params_chi1, \"chi12\": params_chi12}\n",
    "    params_all_voltages.append(params_voltage)\n",
    "\n",
    "print(f\"\\n✓ Processed all {len(params_all_voltages)} voltages\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved Figure 18 data to /Users/krzywdaja/Documents/spatial-correlations-conveyor/data_analysis/protection_code_repo/processed_data/fig18/data.json\n",
      "  Number of voltages: 3\n",
      "  Voltage labels: ['65mV', '75mV', '85mV']\n"
     ]
    }
   ],
   "source": [
    "# Save processed data for Figure 18 (voltage comparison plot)\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create output directory\n",
    "processed_data_path = Path().resolve().parent / 'processed_data' / 'fig18'\n",
    "processed_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert any numpy types to native python types so JSON serializes cleanly\n",
    "def make_json_serializable(d):\n",
    "    import numpy as np\n",
    "    if isinstance(d, dict):\n",
    "        return {k: make_json_serializable(v) for k, v in d.items()}\n",
    "    elif isinstance(d, np.ndarray):\n",
    "        return d.tolist()\n",
    "    elif isinstance(d, (np.integer, np.floating)):\n",
    "        return d.item()\n",
    "    elif isinstance(d, list):\n",
    "        return [make_json_serializable(x) for x in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "# Prepare data to save\n",
    "data_to_save = {\n",
    "    \"params_all_voltages\": params_all_voltages,\n",
    "    \"voltage_labels\": voltage_labels,  # ['65mV', '75mV', '85mV']\n",
    "    \"description\": \"Processed two-point correlation parameters for three voltages (65mV, 75mV, 85mV) for Figure 18\",\n",
    "    \"note\": \"Each voltage has chi1 (9 positions) and chi12 (8 positions) parameters\"\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "save_path = processed_data_path / 'data.json'\n",
    "with open(save_path, \"w\") as f:\n",
    "    json.dump(make_json_serializable(data_to_save), f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved Figure 18 data to {save_path}\")\n",
    "print(f\"  Number of voltages: {len(params_all_voltages)}\")\n",
    "print(f\"  Voltage labels: {voltage_labels}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
